{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khairul-islam99/Qwen3_VL_8B_OCR/blob/main/Qwen3_VL_8B_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nZaYwbljSeM8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdhtrSRMShsw"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "# Verify if the model is already active in the current session\n",
        "if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "    print(\"[INFO] Model not detected in active memory. Initializing download and load sequence...\")\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        \"unsloth/Qwen3-VL-8B-Instruct\",\n",
        "        load_in_4bit = False,\n",
        "        load_in_8bit = True,\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "    )\n",
        "    print(\"[SUCCESS] Model successfully loaded and ready for inference!\")\n",
        "else:\n",
        "    print(\"[INFO] Model is already present in memory. Skipping initialization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjzaiinGqUFJ"
      },
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2hprVFXWPdt"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EeT_RRf8XxI7"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import io\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer\n",
        "\n",
        "def format_chat_history(chat_history):\n",
        "    \"\"\"\n",
        "    Converts the Gradio chat history (a list of tuples) into the\n",
        "    standardized list of dictionaries expected by the generation model.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    if chat_history is None:\n",
        "        return messages\n",
        "\n",
        "    for turn in chat_history:\n",
        "        user_msg, assistant_msg = turn\n",
        "        messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_msg}]})\n",
        "        if assistant_msg:\n",
        "             messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_msg}]})\n",
        "\n",
        "    return messages\n",
        "\n",
        "def chat_stream(image_input, text_input, chat_history):\n",
        "    \"\"\"\n",
        "    A generator function that processes inputs and streams the model's response\n",
        "    back to the user interface in real-time.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Format the history and current prompt\n",
        "    messages = format_chat_history(chat_history)\n",
        "\n",
        "    # 2. Prepare the new user prompt\n",
        "    new_user_content = []\n",
        "    pil_image = None\n",
        "\n",
        "    if image_input is not None:\n",
        "        pil_image = image_input\n",
        "        new_user_content.append({\"type\": \"image\"})\n",
        "\n",
        "    if text_input:\n",
        "        new_user_content.append({\"type\": \"text\", \"text\": text_input})\n",
        "    else:\n",
        "        if image_input is not None:\n",
        "             new_user_content.append({\"type\": \"text\", \"text\": \"Extract text from this image.\"})\n",
        "             text_input = \"Extract text from this image.\"\n",
        "        else:\n",
        "            yield chat_history\n",
        "            return\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": new_user_content})\n",
        "\n",
        "    # 3. Apply the chat template\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt = True\n",
        "    )\n",
        "\n",
        "    # 4. Prepare inputs for the model\n",
        "    inputs = tokenizer(\n",
        "        pil_image,\n",
        "        input_text,\n",
        "        add_special_tokens = False,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # 5. Setup the text iterator streamer\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generation_kwargs = dict(\n",
        "        inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=8192,\n",
        "        use_cache=True,\n",
        "        temperature=0.2,\n",
        "        min_p=0.1\n",
        "    )\n",
        "\n",
        "    # 6. Initialize generation in a separate background thread\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    # 7. Yield new tokens iteratively to the Gradio UI\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    chat_history.append((text_input, \"\"))\n",
        "\n",
        "    response_text = \"\"\n",
        "    for new_token in streamer:\n",
        "        response_text += new_token\n",
        "        chat_history[-1] = (text_input, response_text)\n",
        "        yield chat_history\n",
        "\n",
        "def clear_chat():\n",
        "    \"\"\"Clears the chat history and resets inputs.\"\"\"\n",
        "    return [], None, \"\"\n",
        "\n",
        "# --- Build the Gradio Interface ---\n",
        "\n",
        "theme = gr.themes.Soft(\n",
        "    primary_hue=\"indigo\",\n",
        "    secondary_hue=\"blue\",\n",
        "    font=gr.themes.GoogleFont(\"Open Sans\")\n",
        ")\n",
        "\n",
        "with gr.Blocks(theme=theme, title=\"Bini AI Assistant\") as demo:\n",
        "    with gr.Column():\n",
        "        # --- Application Header ---\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center; margin-bottom: 1rem;\">\n",
        "                <h2>ü§ñ Bini AI Assistant üíª</h2>\n",
        "                <p>Advanced Multimodal Inference Interface</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # --- Chat Window ---\n",
        "        chatbot = gr.Chatbot(\n",
        "            value=[[None, \"Hello! I am Bini, a multimodal AI assistant developed by Md Khairul Islam. How may I assist you today?\"]],\n",
        "            label=\"Interaction Log\",\n",
        "            height=400,\n",
        "            bubble_full_width=False,\n",
        "        )\n",
        "\n",
        "        # --- Input Section ---\n",
        "        with gr.Row(equal_height=False):\n",
        "            with gr.Column(scale=4, min_width=200):\n",
        "                image_box = gr.Image(\n",
        "                    type=\"pil\",\n",
        "                    label=\"Upload Image Reference (Optional)\",\n",
        "                    sources=[\"upload\"],\n",
        "                    height=200\n",
        "                )\n",
        "            with gr.Column(scale=6):\n",
        "                text_box = gr.Textbox(\n",
        "                    label=\"Message\",\n",
        "                    placeholder=\"Enter your prompt or ask a question regarding the uploaded image...\",\n",
        "                    show_label=False,\n",
        "                    lines=4\n",
        "                )\n",
        "\n",
        "        # --- Action Buttons ---\n",
        "        with gr.Row():\n",
        "            clear_btn = gr.Button(\"Clear Context\", variant=\"stop\")\n",
        "            send_btn = gr.Button(\"Submit Request\", variant=\"primary\")\n",
        "\n",
        "        # --- Example Prompts ---\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"Please transcribe all the text visible in this image.\"],\n",
        "                [\"Analyze this image and describe its primary contents.\"],\n",
        "                [\"Extract the data from this image and format it as a Markdown table.\"],\n",
        "            ],\n",
        "            inputs=[text_box],\n",
        "            label=\"Suggested Prompts\"\n",
        "        )\n",
        "\n",
        "        # --- Footer ---\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center; font-size: 0.85em; color: gray; margin-top: 2rem;\">\n",
        "                <p><b>Bini AI</b> was developed by <b>Md Khairul Islam</b>.<br>\n",
        "                Powered by unsloth/Qwen3-VL-8B-Instruct (4-bit quantization)</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    # --- Event Handler Routines ---\n",
        "\n",
        "    # Triggered via Send Button\n",
        "    send_btn.click(\n",
        "        chat_stream,\n",
        "        inputs=[image_box, text_box, chatbot],\n",
        "        outputs=[chatbot],\n",
        "    ).then(\n",
        "        lambda: (None, \"\"),\n",
        "        outputs=[image_box, text_box]\n",
        "    )\n",
        "\n",
        "    # Triggered via Enter Key\n",
        "    text_box.submit(\n",
        "        chat_stream,\n",
        "        inputs=[image_box, text_box, chatbot],\n",
        "        outputs=[chatbot],\n",
        "    ).then(\n",
        "        lambda: (None, \"\"),\n",
        "        outputs=[image_box, text_box]\n",
        "    )\n",
        "\n",
        "    # Triggered via Clear Button\n",
        "    clear_btn.click(\n",
        "        clear_chat,\n",
        "        outputs=[chatbot, image_box, text_box]\n",
        "    )\n",
        "\n",
        "# Launch the interactive application\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtQ_rR1eqZ-y"
      },
      "source": [
        "### Fast_API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWloHQ7bwPid"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn python-multipart pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKxvtrbAqjew"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "import asyncio\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from PIL import Image\n",
        "import io\n",
        "import torch\n",
        "\n",
        "# --- 1. Ngrok Network Configuration ---\n",
        "NGROK_TOKEN = \"Paste Token\" # Developer authentication token\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# --- 2. FastAPI Application Initialization ---\n",
        "app = FastAPI(title=\"Qwen Unified OCR API Engine\")\n",
        "\n",
        "# Implement CORS Middleware to prevent cross-origin resource sharing errors\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Permit all external origins for testing\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Permit all HTTP methods (GET, POST, etc.)\n",
        "    allow_headers=[\"*\"],  # Permit all headers\n",
        ")\n",
        "\n",
        "@app.post(\"/extract-text\")\n",
        "def extract_text(file: UploadFile = File(...)):\n",
        "    global torch, tokenizer, model\n",
        "\n",
        "    # Log incoming requests to the console\n",
        "    print(f\"\\n[INFO] Incoming Request Detected. Target File: {file.filename}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Read and convert the uploaded image\n",
        "        image_bytes = file.file.read()\n",
        "        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "        prompt = \"\"\"You are an expert OCR assistant. Your sole task is to extract all text, data, and mathematical equations from the image exactly as they appear.\n",
        "\n",
        "Follow these strict rules:\n",
        "1. ZERO HALLUCINATION & NO SOLVING: DO NOT solve mathematical equations, DO NOT answer questions, DO NOT translate, and DO NOT summarize. Just transcribe exactly what is visible.\n",
        "2. BLANKS & EMPTY BOXES: If there is an empty box, blank line, or empty table cell, represent it exactly as [ ] without guessing the content.\n",
        "3. BENGALI TEXT & NUMBERS: Pay extreme attention to Bengali numerals. Strictly differentiate between '‡ß™' (four) and '‡ßÆ' (eight), and '‡ß≠' (seven) and '‡ßß' (one). Ensure 100% accuracy for complex conjuncts (‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶æ‡¶ï‡ßç‡¶∑‡¶∞) and correct punctuation (like the '‡¶¶‡¶æ‡¶Å‡ßú‡¶ø' | ).\n",
        "4. MATH & SYMBOLS: Use standard LaTeX formatting for all mathematical equations, fractions, superscripts, and subscripts. Keep geometry labels exact. Keep question numbers exact.\n",
        "5. TABLES: Recreate all tables using strict Markdown formatting (| Column | Column |). Do not skip empty columns.\n",
        "6. ENGLISH & CODE: Maintain strict case sensitivity (uppercase/lowercase). Do not auto-correct typos present in the image. If there is computer code, wrap it in triple backticks (```) and preserve exact spacing.\n",
        "7. LAYOUT: Preserve original line breaks, stanzas, paragraphs, page numbers, and bullet points exactly as seen.\"\"\"\n",
        "\n",
        "        msg_content = [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]\n",
        "        messages = [{\"role\": \"user\", \"content\": msg_content}]\n",
        "\n",
        "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = tokenizer(pil_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        print(\"[INFO] Initializing inference via Qwen architecture. Processing in progress...\")\n",
        "\n",
        "        # Enforce strict inference parameters for accurate OCR\n",
        "        with torch.inference_mode():\n",
        "            output_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=2048,\n",
        "                use_cache=True,\n",
        "                temperature=0.1,\n",
        "                min_p=0.1\n",
        "            )\n",
        "\n",
        "        generated_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, output_ids)]\n",
        "        extracted_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[SUCCESS] Extraction payload compiled successfully in {process_time:.2f} seconds.\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"extracted_text\": extracted_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] An exception occurred during execution: {str(e)}\")\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "# --- 3. Background Server Configuration ---\n",
        "class BackgroundUvicorn(threading.Thread):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(daemon=True)\n",
        "        self.server = uvicorn.Server(config)\n",
        "\n",
        "    def run(self):\n",
        "        # Establish an isolated event loop for the Uvicorn server thread\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        loop.run_until_complete(self.server.serve())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Terminate existing tunnels to prevent conflicts\n",
        "    ngrok.kill()\n",
        "    tunnel = ngrok.connect(8000)\n",
        "\n",
        "    print(\"=\"*75)\n",
        "    print(f\"üöÄ PRIMARY API ENDPOINT: {tunnel.public_url}/extract-text\")\n",
        "    print(f\"üìÑ INTERACTIVE API DOCUMENTATION (Swagger UI): {tunnel.public_url}/docs\")\n",
        "    print(\"=\"*75)\n",
        "\n",
        "    # Boot up the server on the background thread\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
        "    server_thread = BackgroundUvicorn(config)\n",
        "    server_thread.start()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}