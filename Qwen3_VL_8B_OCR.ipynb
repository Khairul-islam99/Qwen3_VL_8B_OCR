{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khairul-islam99/Qwen3_VL_8B_OCR/blob/main/Qwen3_VL_8B_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_K9TAMh89jK"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nZaYwbljSeM8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdhtrSRMShsw"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "\n",
        "# Verify if the model is already active in the current session\n",
        "if 'model' not in globals() or 'tokenizer' not in globals():\n",
        "    print(\"[INFO] Model not detected in active memory. Initializing download and load sequence...\")\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        \"unsloth/Qwen3-VL-8B-Instruct\",\n",
        "        load_in_4bit = False,\n",
        "        load_in_8bit = True,\n",
        "        use_gradient_checkpointing = \"unsloth\",\n",
        "    )\n",
        "    print(\"[SUCCESS] Model successfully loaded and ready for inference!\")\n",
        "else:\n",
        "    print(\"[INFO] Model is already present in memory. Skipping initialization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjzaiinGqUFJ"
      },
      "source": [
        "### Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2hprVFXWPdt"
      },
      "outputs": [],
      "source": [
        "!pip install gradio PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EeT_RRf8XxI7"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import io\n",
        "from threading import Thread\n",
        "from transformers import TextIteratorStreamer\n",
        "import fitz\n",
        "\n",
        "# --- THE UPGRADED MASTER PROMPT (STRICT EDITION) ---\n",
        "MASTER_PROMPT = \"\"\"You are a highly precise OCR assistant. Extract EVERYTHING from the provided image with 100% accuracy. Output ONLY the extracted text. Do not include any conversational filler.\n",
        "\n",
        "STRICT GUIDELINES:\n",
        "1. ZERO HALLUCINATION & NO SOLVING (CRITICAL): Transcribe exactly what is visible. DO NOT calculate missing math answers, DO NOT solve equations, and DO NOT auto-complete patterns.\n",
        "2. BLANKS & BOXES (CRITICAL): If a table cell is empty or there is a blank space, you MUST write exactly `[ ]`. DO NOT fill in the blanks with your own calculations (e.g., do not calculate squares for missing cells).\n",
        "3. TABLES (CRITICAL): You MUST recreate all tables using strict Markdown pipe formatting (e.g., `| Column 1 | Column 2 |`). Do not use raw tabs or spaces. Any table not using `|` is a failure.\n",
        "4. BENGALI ACCURACY: Pay extreme attention to Bengali numerals. Strictly differentiate '‡ß™' (four) from '‡ßÆ' (eight), and '‡ß≠' (seven) from '‡ßß' (one). Preserve complex conjuncts (‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶æ‡¶ï‡ßç‡¶∑‡¶∞) and Bengali punctuation.\n",
        "5. CAPTIONS: You must extract all headers, footers, page numbers, and image captions exactly as they appear in the original layout.\n",
        "6. MATH & SYMBOLS: Use standard LaTeX formatting for equations, fractions, superscripts, and subscripts.\n",
        "7. CODE & ENGLISH: Maintain strict case sensitivity. Do not auto-correct typos.\n",
        "8. LAYOUT: Replicate the original line breaks, paragraphs, and bullet points exactly as seen.\"\"\"\n",
        "\n",
        "def format_chat_history(chat_history):\n",
        "    messages = []\n",
        "    if chat_history is None:\n",
        "        return messages\n",
        "    for turn in chat_history:\n",
        "        user_msg, assistant_msg = turn\n",
        "        messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_msg}]})\n",
        "        if assistant_msg:\n",
        "             messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_msg}]})\n",
        "    return messages\n",
        "\n",
        "def chat_stream(file_input, text_input, chat_history):\n",
        "    messages = format_chat_history(chat_history)\n",
        "    new_user_content = []\n",
        "    pil_image = None\n",
        "\n",
        "    if file_input is not None:\n",
        "        file_path = file_input\n",
        "\n",
        "        # Check if uploaded file is a PDF\n",
        "        if file_path.lower().endswith(\".pdf\"):\n",
        "            print(\"[INFO] PDF file detected in Gradio. Converting page 0 to image...\")\n",
        "            pdf_doc = fitz.open(file_path)\n",
        "            page = pdf_doc.load_page(0)\n",
        "            pix = page.get_pixmap(dpi=300)\n",
        "            pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "            pdf_doc.close()\n",
        "        else:\n",
        "            print(\"[INFO] Standard image detected in Gradio. Processing directly...\")\n",
        "            pil_image = Image.open(file_path).convert(\"RGB\")\n",
        "\n",
        "        new_user_content.append({\"type\": \"image\"})\n",
        "\n",
        "    if text_input:\n",
        "        new_user_content.append({\"type\": \"text\", \"text\": text_input})\n",
        "    else:\n",
        "        if file_input is not None:\n",
        "             new_user_content.append({\"type\": \"text\", \"text\": MASTER_PROMPT})\n",
        "             text_input = \"Executing Strict Master OCR Prompt...\"\n",
        "        else:\n",
        "            yield chat_history\n",
        "            return\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": new_user_content})\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = tokenizer(pil_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # --- DETERMINISTIC SETTINGS (100% NO SAMPLING) ---\n",
        "    generation_kwargs = dict(\n",
        "        inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=8192,\n",
        "        use_cache=True,\n",
        "        do_sample=False,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        min_p=None\n",
        "    )\n",
        "\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    chat_history.append((text_input, \"\"))\n",
        "    response_text = \"\"\n",
        "    for new_token in streamer:\n",
        "        response_text += new_token\n",
        "        chat_history[-1] = (text_input, response_text)\n",
        "        yield chat_history\n",
        "\n",
        "def clear_chat():\n",
        "    return [], None, \"\"\n",
        "\n",
        "# --- Build the Gradio Interface ---\n",
        "theme = gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"blue\", font=gr.themes.GoogleFont(\"Open Sans\"))\n",
        "\n",
        "with gr.Blocks(theme=theme, title=\"Bini AI Assistant\") as demo:\n",
        "    with gr.Column():\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center; margin-bottom: 1rem;\">\n",
        "                <h2>ü§ñ Bini AI Assistant üíª</h2>\n",
        "                <p>Advanced Multimodal OCR Interface (Supports Image & PDF)</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "        chatbot = gr.Chatbot(\n",
        "            value=[[None, \"Hello! I am Bini, a highly precise OCR assistant. Upload an image or PDF, and I will extract the text exactly as it appears.\"]],\n",
        "            label=\"Interaction Log\",\n",
        "            height=400,\n",
        "            bubble_full_width=False,\n",
        "        )\n",
        "        with gr.Row(equal_height=False):\n",
        "            with gr.Column(scale=4, min_width=200):\n",
        "                file_box = gr.File(file_types=[\"image\", \".pdf\"], label=\"Upload Image or PDF\", height=200)\n",
        "            with gr.Column(scale=6):\n",
        "                text_box = gr.Textbox(\n",
        "                    label=\"Message\",\n",
        "                    placeholder=\"Leave blank to use the Master OCR Prompt, or type a custom question...\",\n",
        "                    show_label=False,\n",
        "                    lines=4\n",
        "                )\n",
        "        with gr.Row():\n",
        "            clear_btn = gr.Button(\"Clear Context\", variant=\"stop\")\n",
        "            send_btn = gr.Button(\"Submit Request\", variant=\"primary\")\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center; font-size: 0.85em; color: gray; margin-top: 2rem;\">\n",
        "                <p><b>Bini AI</b> was developed by <b>Md Khairul Islam</b>.<br>\n",
        "                Powered by unsloth/Qwen3-VL-8B-Instruct</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    send_btn.click(chat_stream, inputs=[file_box, text_box, chatbot], outputs=[chatbot]).then(lambda: (None, \"\"), outputs=[file_box, text_box])\n",
        "    text_box.submit(chat_stream, inputs=[file_box, text_box, chatbot], outputs=[chatbot]).then(lambda: (None, \"\"), outputs=[file_box, text_box])\n",
        "    clear_btn.click(clear_chat, outputs=[chatbot, file_box, text_box])\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtQ_rR1eqZ-y"
      },
      "source": [
        "### Fast_API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWloHQ7bwPid"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn python-multipart pyngrok PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKxvtrbAqjew"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "import asyncio\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from PIL import Image\n",
        "import io\n",
        "import torch\n",
        "import fitz\n",
        "\n",
        "# --- 1. Ngrok Network Configuration ---\n",
        "NGROK_TOKEN = \"paste Token\" # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ü‡ßã‡¶ï‡ßá‡¶®\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# --- 2. FastAPI Application Initialization ---\n",
        "app = FastAPI(title=\"Qwen Unified OCR API Engine\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/extract-text\")\n",
        "def extract_text(file: UploadFile = File(...)):\n",
        "    global torch, tokenizer, model\n",
        "\n",
        "    print(f\"\\n[INFO] Incoming Request Detected. Target File: {file.filename}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        file_bytes = file.file.read()\n",
        "\n",
        "        # --- PDF & Image Logic ---\n",
        "        if file.filename.lower().endswith(\".pdf\") or file.content_type == \"application/pdf\":\n",
        "            print(\"[INFO] PDF file detected. Converting the first page to a high-resolution image...\")\n",
        "            pdf_document = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
        "            page = pdf_document.load_page(0)\n",
        "            pix = page.get_pixmap(dpi=300)\n",
        "            pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "            pdf_document.close()\n",
        "        else:\n",
        "            pil_image = Image.open(io.BytesIO(file_bytes)).convert(\"RGB\")\n",
        "\n",
        "        # --- THE UPGRADED MASTER PROMPT (STRICT EDITION) ---\n",
        "        prompt = \"\"\"You are a highly precise OCR assistant. Extract EVERYTHING from the provided image with 100% accuracy. Output ONLY the extracted text. Do not include any conversational filler.\n",
        "\n",
        "STRICT GUIDELINES:\n",
        "1. ZERO HALLUCINATION & NO SOLVING (CRITICAL): Transcribe exactly what is visible. DO NOT calculate missing math answers, DO NOT solve equations, and DO NOT auto-complete patterns.\n",
        "2. BLANKS & BOXES (CRITICAL): If a table cell is empty or there is a blank space, you MUST write exactly `[ ]`. DO NOT fill in the blanks with your own calculations (e.g., do not calculate squares for missing cells).\n",
        "3. TABLES (CRITICAL): You MUST recreate all tables using strict Markdown pipe formatting (e.g., `| Column 1 | Column 2 |`). Do not use raw tabs or spaces. Any table not using `|` is a failure.\n",
        "4. BENGALI ACCURACY: Pay extreme attention to Bengali numerals. Strictly differentiate '‡ß™' (four) from '‡ßÆ' (eight), and '‡ß≠' (seven) from '‡ßß' (one). Preserve complex conjuncts (‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶æ‡¶ï‡ßç‡¶∑‡¶∞) and Bengali punctuation.\n",
        "5. CAPTIONS: You must extract all headers, footers, page numbers, and image captions exactly as they appear in the original layout.\n",
        "6. MATH & SYMBOLS: Use standard LaTeX formatting for equations, fractions, superscripts, and subscripts.\n",
        "7. CODE & ENGLISH: Maintain strict case sensitivity. Do not auto-correct typos.\n",
        "8. LAYOUT: Replicate the original line breaks, paragraphs, and bullet points exactly as seen.\"\"\"\n",
        "\n",
        "        msg_content = [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]\n",
        "        messages = [{\"role\": \"user\", \"content\": msg_content}]\n",
        "\n",
        "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = tokenizer(pil_image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        print(\"[INFO] Initializing inference via Qwen architecture. Processing in progress...\")\n",
        "\n",
        "        # --- DETERMINISTIC SETTINGS (100% NO SAMPLING) ---\n",
        "        with torch.inference_mode():\n",
        "            output_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=2048,\n",
        "                use_cache=True,\n",
        "                do_sample=False,\n",
        "                temperature=None,\n",
        "                top_p=None,\n",
        "                min_p=None\n",
        "            )\n",
        "\n",
        "        generated_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, output_ids)]\n",
        "        extracted_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        process_time = time.time() - start_time\n",
        "        print(f\"[SUCCESS] Extraction payload compiled successfully in {process_time:.2f} seconds.\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"extracted_text\": extracted_text\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] An exception occurred during execution: {str(e)}\")\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n",
        "# --- 3. Background Server Configuration ---\n",
        "class BackgroundUvicorn(threading.Thread):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(daemon=True)\n",
        "        self.server = uvicorn.Server(config)\n",
        "\n",
        "    def run(self):\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        loop.run_until_complete(self.server.serve())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ngrok.kill()\n",
        "    tunnel = ngrok.connect(8000)\n",
        "\n",
        "    print(\"=\"*75)\n",
        "    print(f\"üöÄ PRIMARY API ENDPOINT: {tunnel.public_url}/extract-text\")\n",
        "    print(f\"üìÑ INTERACTIVE API DOCUMENTATION (Swagger UI): {tunnel.public_url}/docs\")\n",
        "    print(\"=\"*75)\n",
        "\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"warning\")\n",
        "    server_thread = BackgroundUvicorn(config)\n",
        "    server_thread.start()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}